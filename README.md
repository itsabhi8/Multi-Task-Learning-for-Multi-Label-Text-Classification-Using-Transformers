# Multi-Task-Learning-for-Multi-Label-Text-Classification-Using-Transformers
This project implements a multi-task learning model for multi-label text classification using BERT. It leverages techniques like early stopping, learning rate warm-up, and gradient accumulation to optimize performance, achieving strong macro and micro F1 scores on a custom dataset.
